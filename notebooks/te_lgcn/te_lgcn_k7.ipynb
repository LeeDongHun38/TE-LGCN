{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxXf2mkJBqJh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fb86c7b2-964e-4716-ed29-7da97534d0e1",
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8oBzettOyJ9",
    "outputId": "70a7c8dd-2d8b-4904-ba56-36412befb073"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# =========================================================\n",
    "# 1. \ud658\uacbd \uc124\uc815 \ubc0f \uc2dc\ub4dc \uace0\uc815\n",
    "# =========================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# [\uacbd\ub85c \uc124\uc815] \ubcf8\uc778\uc758 \ud658\uacbd\uc5d0 \ub9de\uac8c \uc218\uc815\ud558\uc138\uc694\n",
    "base_path = \"/content/drive/MyDrive/unstructured\"\n",
    "data_path = f\"{base_path}/k5_filtered\" # Split\ub41c \ub370\uc774\ud130\uac00 \uc788\ub294 \ud3f4\ub354\n",
    "movie_meta_path = f\"{base_path}/movie_data_final_clean.csv\" # \uc904\uac70\ub9ac\uac00 \uc788\ub294 \uc601\ud654 \uba54\ud0c0\ub370\uc774\ud130\n",
    "topic_edge_path = f\"{base_path}/topic_vectors_7.csv\" # \ud1a0\ud53d \ub370\uc774\ud130"
   ],
   "metadata": {
    "id": "3AWR2qVNSYKX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "01b51584-6f33-438a-c475-60fc50bf3b70"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# 2. \ub370\uc774\ud130 \ub85c\ub4dc (Pre-split Data)\n",
    "# =========================================================\n",
    "print(\"\\n\ud83d\udcc2 \ub370\uc774\ud130 \ub85c\ub4dc \uc911...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    val_df   = pd.read_csv(f\"{data_path}/val.csv\")\n",
    "    test_df  = pd.read_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "    with open(f\"{data_path}/user2idx.pkl\", 'rb') as f: user2idx = pickle.load(f)\n",
    "    with open(f\"{data_path}/item2idx.pkl\", 'rb') as f: item2idx = pickle.load(f)\n",
    "\n",
    "    n_users = len(user2idx)\n",
    "    n_items = len(item2idx)\n",
    "    print(f\"\u2705 \uae30\ubcf8 \ub370\uc774\ud130 \ub85c\ub4dc \uc644\ub8cc. User: {n_users}, Item: {n_items}\")\n",
    "\n",
    "    # Topic \ub370\uc774\ud130 \ub85c\ub4dc\n",
    "    topic_df = pd.read_csv(topic_edge_path)\n",
    "    n_topics = topic_df['topic_id'].nunique()\n",
    "    print(f\"\u2705 \ud1a0\ud53d \ub370\uc774\ud130 \ub85c\ub4dc \uc644\ub8cc. Topic \uc218: {n_topics}, \uc5f0\uacb0 \uc218: {len(topic_df)}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\u274c Error: \ud30c\uc77c\uc744 \ucc3e\uc744 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. {e}\")\n",
    "    raise"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrH7tqI20nqD",
    "outputId": "ca750729-cfe0-416a-cc92-5d18961655f0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\udcc2 \ub370\uc774\ud130 \ub85c\ub4dc \uc911...\n",
      "\u2705 \uae30\ubcf8 \ub370\uc774\ud130 \ub85c\ub4dc \uc644\ub8cc. User: 671, Item: 3485\n",
      "\u2705 \ud1a0\ud53d \ub370\uc774\ud130 \ub85c\ub4dc \uc644\ub8cc. Topic \uc218: 7, \uc5f0\uacb0 \uc218: 21029\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# [\ucd94\uac00] \ud1a0\ud53d \ub370\uc774\ud130 ID \uc7ac\ub9e4\ud551 \ubc0f \ud544\ud130\ub9c1\n",
    "# =========================================================\n",
    "print(\"\ud83d\udd04 \ud1a0\ud53d \ub370\uc774\ud130 ID \uc7ac\ub9e4\ud551 \uc911...\")\n",
    "\n",
    "# 1. LDA\uc5d0 \uc0ac\uc6a9\ub41c \uc6d0\ubcf8 \uc601\ud654 \ub9ac\uc2a4\ud2b8 \ub85c\ub4dc (\uc21c\uc11c \uc911\uc694!)\n",
    "# LDA\ub294 movie_meta_path \ud30c\uc77c\uc758 \uc21c\uc11c\ub300\ub85c 0, 1, 2... ID\ub97c \ubd80\uc5ec\ud588\uc74c\n",
    "lda_movie_source = pd.read_csv(movie_meta_path)\n",
    "\n",
    "# 2. {LDA_Index : Original_Movie_ID} \ub9e4\ud551 \uc0dd\uc131\n",
    "# (LDA \uacb0\uacfc\uc758 movie_id\ub294 \uc704 \ud30c\uc77c\uc758 \ud589 \ubc88\ud638(Index)\uc640 \uac19\uc2b5\ub2c8\ub2e4)\n",
    "lda_idx_to_real_id = lda_movie_source['movieId'].to_dict()\n",
    "\n",
    "# 3. topic_df\uc758 'movie_id'(LDA Index)\ub97c 'real_movie_id'\ub85c \ubcc0\ud658\n",
    "topic_df['real_movie_id'] = topic_df['movie_id'].map(lda_idx_to_real_id)\n",
    "\n",
    "# 4. {Original_Movie_ID : LightGCN_Index} \ub9e4\ud551 (item2idx \uc0ac\uc6a9)\n",
    "# item2idx\ub294 LightGCN \ub85c\ub4dc \uc2dc pickle\uc5d0\uc11c \ubd88\ub7ec\uc628 \uac83\n",
    "topic_df['lightgcn_item_idx'] = topic_df['real_movie_id'].map(item2idx)\n",
    "\n",
    "# 5. LightGCN \ub370\uc774\ud130\uc14b\uc5d0 \uc5c6\ub294 \uc601\ud654(NaN) \uc81c\uac70 (K-core \ud544\ud130\ub9c1\ub41c \uac83\ub4e4 \uc0ad\uc81c)\n",
    "print(f\"\ub9e4\ud551 \uc804 \ud1a0\ud53d \uc5f0\uacb0 \uc218: {len(topic_df)}\")\n",
    "topic_df = topic_df.dropna(subset=['lightgcn_item_idx']) # \uc5c6\ub294 \uc601\ud654 \uc0ad\uc81c\n",
    "topic_df['lightgcn_item_idx'] = topic_df['lightgcn_item_idx'].astype(int) # \uc815\uc218\ud615 \ubcc0\ud658\n",
    "\n",
    "print(f\"\ub9e4\ud551 \ud6c4 \ud1a0\ud53d \uc5f0\uacb0 \uc218: {len(topic_df)}\")\n",
    "\n",
    "# 6. \uceec\ub7fc \uc774\ub984 \uad50\uccb4 (\uadf8\ub798\ud504 \uc0dd\uc131 \ud568\uc218 \ud638\ud658\uc131 \uc704\ud574)\n",
    "# \uae30\uc874 'movie_id'\ub294 \ubc84\ub9ac\uace0, \ub9e4\ud551\ub41c ID\ub97c 'movie_id'\ub85c \uc0ac\uc6a9\n",
    "topic_df = topic_df[['lightgcn_item_idx', 'topic_id', 'related']].rename(columns={'lightgcn_item_idx': 'movie_id'})\n",
    "\n",
    "print(\"\u2705 \ud1a0\ud53d \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uc644\ub8cc. (LightGCN ID\uc640 \ub3d9\uae30\ud654\ub428)\")\n",
    "print(topic_df.head())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nna8a3jB1w7N",
    "outputId": "21757d30-fd26-4fa2-b4c9-c928e2f020fd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udd04 \ud1a0\ud53d \ub370\uc774\ud130 ID \uc7ac\ub9e4\ud551 \uc911...\n",
      "\ub9e4\ud551 \uc804 \ud1a0\ud53d \uc5f0\uacb0 \uc218: 21029\n",
      "\ub9e4\ud551 \ud6c4 \ud1a0\ud53d \uc5f0\uacb0 \uc218: 8122\n",
      "\u2705 \ud1a0\ud53d \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uc644\ub8cc. (LightGCN ID\uc640 \ub3d9\uae30\ud654\ub428)\n",
      "   movie_id  topic_id   related\n",
      "0       431         1  0.960950\n",
      "1      1014         1  0.603947\n",
      "2      1014         6  0.146624\n",
      "3      1014         0  0.115764\n",
      "4      1014         5  0.113900\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Load Pre-trained Doc2Vec Embeddings\n",
    "# =========================================================\n",
    "print(\"\\n\ud83d\udce5 Loading Doc2Vec embeddings...\")\n",
    "doc2vec_path = f\"{base_path}/doc2vec_embeddings_64d.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(doc2vec_path, 'rb') as f:\n",
    "        doc2vec_data = pickle.load(f)\n",
    "\n",
    "    # Validate consistency\n",
    "    assert doc2vec_data['n_items'] == n_items, \\\n",
    "        f\"Item count mismatch! Doc2Vec: {doc2vec_data['n_items']}, Current: {n_items}\"\n",
    "    assert doc2vec_data['item2idx'] == item2idx, \\\n",
    "        \"item2idx mapping mismatch! Doc2Vec and LDA must use same data split.\"\n",
    "\n",
    "    doc2vec_weights = torch.FloatTensor(doc2vec_data['embeddings']).to(device)\n",
    "    print(f\"\u2705 Doc2Vec embeddings loaded successfully\")\n",
    "    print(f\"   Shape: {doc2vec_weights.shape}, Created: {doc2vec_data['created_at']}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\u26a0\ufe0f  Doc2Vec embeddings not found. Falling back to random initialization\")\n",
    "    doc2vec_weights = None\n",
    "except AssertionError as e:\n",
    "    print(f\"\u274c Validation Error: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# 3. \ub370\uc774\ud130 \ubd84\ub9ac & \ub515\uc154\ub108\ub9ac \uc0dd\uc131\n",
    "# =========================================================\n",
    "# 1) Positive Data (4\uc810 \uc774\uc0c1): \uadf8\ub798\ud504 \ubc0f \uc815\ub2f5\uc6a9\n",
    "train_pos_df = train_df[train_df['rating'] >= 4.0].copy()\n",
    "\n",
    "# 2) Hard Negative Data (2\uc810 \uc774\ud558): \uc624\ub2f5 \ud559\uc2b5\uc6a9\n",
    "train_neg_df = train_df[train_df['rating'] <= 2.0].copy()\n",
    "\n",
    "# \ud559\uc2b5\uc6a9 \ub515\uc154\ub108\ub9ac\n",
    "user_pos_items = train_pos_df.groupby(\"user\")[\"item\"].apply(set).to_dict()\n",
    "user_hard_neg_items = train_neg_df.groupby(\"user\")[\"item\"].apply(list).to_dict()\n",
    "\n",
    "print(f\"Positive Interactions: {len(train_pos_df)}\")\n",
    "print(f\"Hard Negative Interactions: {len(train_neg_df)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9uvqQTA0pf0",
    "outputId": "008c176d-553b-471f-c4b3-7541b0f40edb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Positive Interactions: 46243\n",
      "Hard Negative Interactions: 11327\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# 4. Feature 2: \uadf8\ub798\ud504 \uad6c\ucd95 (User-Item-Topic)\n",
    "# =========================================================\n",
    "\n",
    "def get_hetero_adj_mat(n_users, n_items, n_topics, user_item_df, item_topic_df):\n",
    "    num_nodes = n_users + n_items + n_topics\n",
    "\n",
    "    u_idx = user_item_df['user'].values\n",
    "    i_idx = user_item_df['item'].values + n_users\n",
    "\n",
    "    it_item_idx = item_topic_df['movie_id'].values + n_users\n",
    "    it_topic_idx = item_topic_df['topic_id'].values + n_users + n_items\n",
    "\n",
    "    all_src = np.concatenate([u_idx, i_idx, it_item_idx, it_topic_idx])\n",
    "    all_dst = np.concatenate([i_idx, u_idx, it_topic_idx, it_item_idx])\n",
    "\n",
    "    data = np.ones(len(all_src))\n",
    "    adj = sp.coo_matrix((data, (all_src, all_dst)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "    norm_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "    indices = torch.LongTensor(np.vstack((norm_adj.row, norm_adj.col)))\n",
    "    values = torch.FloatTensor(norm_adj.data)\n",
    "\n",
    "    return torch.sparse_coo_tensor(indices, values, torch.Size(norm_adj.shape)).to(device)\n",
    "\n",
    "print(\"\\n\ud83d\udd78\ufe0f \uc774\uc885 \uadf8\ub798\ud504(Heterogeneous Graph) \uc0dd\uc131 \uc911...\")\n",
    "# topic_df\ub294 \uc55e\uc5d0\uc11c \ub85c\ub4dc\ud55c \ub370\uc774\ud130\n",
    "Adj_Matrix = get_hetero_adj_mat(n_users, n_items, n_topics, train_pos_df, topic_df)\n",
    "print(f\"\u2705 \uadf8\ub798\ud504 \uc0dd\uc131 \uc644\ub8cc! (Nodes: {n_users+n_items+n_topics})\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQR1JWkY1T1j",
    "outputId": "ea583645-9e4a-4356-a1a4-8122991fb65e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\udd78\ufe0f \uc774\uc885 \uadf8\ub798\ud504(Heterogeneous Graph) \uc0dd\uc131 \uc911...\n",
      "\u2705 \uadf8\ub798\ud504 \uc0dd\uc131 \uc644\ub8cc! (Nodes: 4163)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-1196641890.py:21: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# 5. \ubaa8\ub378 \uc815\uc758: LDA-Only LightGCN (\uc218\uc815\ub428)\n",
    "# =========================================================\n",
    "class LDALightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_topics, dim, layers, A_hat, doc2vec_weights=None):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_topics = n_topics\n",
    "        self.dim = dim\n",
    "        self.layers = layers\n",
    "        self.A_hat = A_hat\n",
    "\n",
    "        # 1. User Embedding (Random)\n",
    "        self.user_emb = nn.Embedding(n_users, dim)\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.1)\n",
    "\n",
    "        # 2. Item Embedding (Doc2Vec Initialization)\n",
    "        if doc2vec_weights is not None:\n",
    "            self.item_emb = nn.Embedding.from_pretrained(doc2vec_weights, freeze=False)\n",
    "            self.register_buffer('fixed_doc2vec', doc2vec_weights.clone().detach())\n",
    "            print(\"   \u2705 Item embeddings initialized with Doc2Vec (trainable)\")\n",
    "        else:\n",
    "            self.item_emb = nn.Embedding(n_items, dim)\n",
    "            nn.init.normal_(self.item_emb.weight, std=0.1)\n",
    "            self.register_buffer('fixed_doc2vec', None)\n",
    "            print(\"   \u26a0\ufe0f  Item embeddings using random initialization\")\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.1)\n",
    "\n",
    "        # 3. Topic Embedding (Random)\n",
    "        self.topic_emb = nn.Embedding(n_topics, dim)\n",
    "        nn.init.normal_(self.topic_emb.weight, std=0.1)\n",
    "\n",
    "    def get_all_embeddings(self):\n",
    "        users = self.user_emb.weight\n",
    "        items = self.item_emb.weight\n",
    "        topics = self.topic_emb.weight\n",
    "\n",
    "        all_emb = torch.cat([users, items, topics], dim=0)\n",
    "        embs = [all_emb]\n",
    "\n",
    "        for _ in range(self.layers):\n",
    "            all_emb = torch.sparse.mm(self.A_hat, all_emb)\n",
    "            embs.append(all_emb)\n",
    "\n",
    "        out = torch.stack(embs, dim=0).mean(dim=0)\n",
    "\n",
    "        users_final = out[:self.n_users]\n",
    "        items_final = out[self.n_users : self.n_users + self.n_items]\n",
    "        # topics_final\uc740 \ud559\uc2b5\uc5d0\ub294 \uc4f0\uc774\uc9c0\ub9cc, \ucd5c\uc885 \ucd94\ucc9c \uc810\uc218 \uacc4\uc0b0\uc5d4 \uc548 \uc4f0\uc784\n",
    "\n",
    "        return users_final, items_final"
   ],
   "metadata": {
    "id": "bEC7mrWP12vK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# 6. \uc720\ud2f8\ub9ac\ud2f0 \ud568\uc218 (\uc0d8\ud50c\ub9c1 & \ud3c9\uac00)\n",
    "# =========================================================\n",
    "def sample_batch_hard_neg(batch_size, user_pos, user_neg, n_items, hard_prob=0.5):\n",
    "    \"\"\"\n",
    "    Hard Negative Sampling\uc744 \uc801\uc6a9\ud55c \ubc30\uce58 \uc0d8\ud50c\ub9c1 \ud568\uc218\n",
    "    \"\"\"\n",
    "    users = np.random.choice(list(user_pos.keys()), size=batch_size)\n",
    "    pos, neg = [], []\n",
    "    for u in users:\n",
    "        # Positive Item Sampling\n",
    "        pos.append(np.random.choice(list(user_pos[u])))\n",
    "\n",
    "        # Negative Item Sampling (Hard Negative Logic)\n",
    "        # \uc720\uc800\uac00 \uc2eb\uc5b4\ud55c \uc544\uc774\ud15c(user_neg)\uc774 \uc788\uace0, \ud655\ub960(0.5)\uc5d0 \uac78\ub9ac\uba74 \uadf8\uac78 \uc120\ud0dd\n",
    "        if (u in user_neg) and (len(user_neg[u]) > 0) and (random.random() < hard_prob):\n",
    "            neg.append(np.random.choice(user_neg[u]))\n",
    "        else:\n",
    "            # \uc544\ub2c8\uba74 Random Negative (\uc548 \ubcf8 \uc601\ud654)\n",
    "            while True:\n",
    "                n = np.random.randint(0, n_items)\n",
    "                if n not in user_pos[u]:\n",
    "                    neg.append(n); break\n",
    "\n",
    "    return torch.LongTensor(users).to(device), torch.LongTensor(pos).to(device), torch.LongTensor(neg).to(device)\n",
    "\n",
    "def evaluate(model, df_eval, k=10):\n",
    "    \"\"\"\n",
    "    \ubaa8\ub378 \uc131\ub2a5 \ud3c9\uac00 \ud568\uc218 (Recall, NDCG \ub4f1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # LDA \ubaa8\ub378\uc740 get_all_embeddings()\uac00 (users, items) \ub450 \uac1c\ub9cc \ubc18\ud658\ud569\ub2c8\ub2e4.\n",
    "    # (Topic \uc784\ubca0\ub529\uc740 \ub0b4\ubd80\uc801\uc73c\ub85c \uc544\uc774\ud15c \uc5c5\ub370\uc774\ud2b8\uc5d0 \uc4f0\uc774\uace0 \ubc18\ud658\uac12\uc5d0\ub294 \ubcf4\ud1b5 \ud3ec\ud568 \uc548 \ub428)\n",
    "    # \ub9cc\uc57d Topic \uc784\ubca0\ub529\ub3c4 \ubc18\ud658\ud558\ub3c4\ub85d \ucf54\ub4dc\ub97c \uc218\uc815\ud588\ub2e4\uba74 \uadf8\uc5d0 \ub9de\uac8c \ubc1b\uc544\uc8fc\uc138\uc694.\n",
    "    # \uc5ec\uae30\uc11c\ub294 \ud45c\uc900\uc801\uc778 (User, Item) \ubc18\ud658\uc744 \uac00\uc815\ud569\ub2c8\ub2e4.\n",
    "\n",
    "    # LDALightGCN \ud074\ub798\uc2a4\uc758 get_all_embeddings\uac00 user, item\ub9cc \ubc18\ud658\ud558\ub294\uc9c0 \ud655\uc778 \ud544\uc694\n",
    "    # \uc55e\uc11c \ub4dc\ub9b0 \ucf54\ub4dc\ub294 user, item \ub450 \uac1c\ub9cc \ubc18\ud658\ud558\ub3c4\ub85d \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n",
    "    users_final, items_final = model.get_all_embeddings()\n",
    "\n",
    "    hits, ndcg, prec, recall, total = 0, 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for u_idx, group in df_eval.groupby('user'):\n",
    "            total += 1\n",
    "            targets = set(group['item'].values)\n",
    "\n",
    "            # \uc810\uc218 \uacc4\uc0b0\n",
    "            scores = torch.matmul(users_final[u_idx], items_final.t())\n",
    "\n",
    "            # Train \ub370\uc774\ud130 \ub9c8\uc2a4\ud0b9 (\uc774\ubbf8 \ubcf8 \uac74 \ucd94\ucc9c \uc81c\uc678)\n",
    "            if u_idx in user_pos_items:\n",
    "                scores[list(user_pos_items[u_idx])] = -1e9\n",
    "\n",
    "            _, topk = torch.topk(scores, k)\n",
    "            topk = topk.cpu().tolist()\n",
    "\n",
    "            num_correct = 0\n",
    "            dcg, idcg = 0.0, 0.0\n",
    "\n",
    "            for i, item in enumerate(topk):\n",
    "                if item in targets:\n",
    "                    num_correct += 1\n",
    "                    dcg += 1.0 / np.log2(i + 2)\n",
    "\n",
    "            for i in range(min(len(targets), k)):\n",
    "                idcg += 1.0 / np.log2(i + 2)\n",
    "\n",
    "            if num_correct > 0: hits += 1\n",
    "            prec += num_correct / k\n",
    "            recall += num_correct / len(targets)\n",
    "            if idcg > 0: ndcg += dcg / idcg\n",
    "\n",
    "    return {'Hit': hits/total, 'Prec': prec/total, 'Recall': recall/total, 'NDCG': ndcg/total}"
   ],
   "metadata": {
    "id": "NnLV8J4U2hXd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# 7. Loss Function\n",
    "# =========================================================\n",
    "def calc_lda_loss(u_f, i_pos_f, i_neg_f, u_0, i_pos_0, i_neg_0, t_0, fixed_vec, lam1, lam2):\n",
    "    # 1. BPR Loss\n",
    "    pos_scores = (u_f * i_pos_f).sum(dim=1)\n",
    "    neg_scores = (u_f * i_neg_f).sum(dim=1)\n",
    "    bpr_loss = torch.mean(torch.nn.functional.softplus(-(pos_scores - neg_scores)))\n",
    "\n",
    "    # 2. Reg Loss (Topic Embedding \ud3ec\ud568!)\n",
    "    # \ud1a0\ud53d \ub178\ub4dc\ub3c4 \ud559\uc2b5 \ud30c\ub77c\ubbf8\ud130\uc774\ubbc0\ub85c \uaddc\uc81c \ub300\uc0c1\uc5d0 \ud3ec\ud568\ud574\uc57c \ud568\n",
    "    reg_loss = 0.5 * (u_0.norm(2).pow(2) + i_pos_0.norm(2).pow(2) +\n",
    "                      i_neg_0.norm(2).pow(2) + t_0.norm(2).pow(2)) / u_0.size(0)\n",
    "\n",
    "\n",
    "    # 3. Content Consistency Loss (NEW)\n",
    "    if fixed_vec is not None:\n",
    "        content_loss = (i_pos_0 - fixed_vec).norm(2).pow(2) / i_pos_0.size(0)\n",
    "    else:\n",
    "        content_loss = torch.tensor(0.0, device=u_0.device)\n",
    "\n",
    "    return bpr_loss + lam1 * reg_loss + lam2 * content_loss\n"
   ],
   "metadata": {
    "id": "a2l2VfLV18EN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# 8. \ud559\uc2b5 \uc2e4\ud589\n",
    "# =========================================================\n",
    "dim = 64\n",
    "layers = 3\n",
    "batch_size = 1024\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "lambda1 = 1e-5  # Base \uc2e4\ud5d8 \ucd5c\uc801\uac12\n",
    "lambda2 = 1e-3  # Content consistency loss weight\n",
    "\n",
    "model = LDALightGCN(n_users, n_items, n_topics, dim, layers, Adj_Matrix,\n",
    "                    doc2vec_weights=doc2vec_weights).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_recall = 0.0\n",
    "best_epoch = 0\n",
    "save_path = \"best_lda_lightgcn.pt\"\n",
    "\n",
    "print(f\"\\n\ud83d\ude80 LDA-Only \ud559\uc2b5 \uc2dc\uc791 (L1={lambda1})...\")\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_pos_df) // batch_size + 1\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        users, pos, neg = sample_batch_hard_neg(batch_size, user_pos_items, user_hard_neg_items, n_items)\n",
    "\n",
    "        # Forward\n",
    "        u_final, i_final = model.get_all_embeddings() # Topic\uc740 \ub0b4\ubd80\uc5d0\uc11c \ucc98\ub9ac\ub428\n",
    "\n",
    "        u_f = u_final[users]\n",
    "        i_pos_f = i_final[pos]\n",
    "        i_neg_f = i_final[neg]\n",
    "\n",
    "        u_0 = model.user_emb.weight[users]\n",
    "        i_pos_0 = model.item_emb.weight[pos]\n",
    "        i_neg_0 = model.item_emb.weight[neg]\n",
    "        t_0 = model.topic_emb.weight\n",
    "\n",
    "        # Loss (Content Loss \uc778\uc790 \uc81c\uac70)\n",
    "\n",
    "        # Extract fixed Doc2Vec vectors for content loss\n",
    "        if model.fixed_doc2vec is not None:\n",
    "            fixed_vec = model.fixed_doc2vec[pos]\n",
    "        else:\n",
    "            fixed_vec = None\n",
    "\n",
    "        loss = calc_lda_loss(u_f, i_pos_f, i_neg_f, u_0, i_pos_0, i_neg_0, t_0,\n",
    "                            fixed_vec, lambda1, lambda2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Evaluation\n",
    "    val_res = evaluate(model, val_df, k=10)\n",
    "    print(f\"[Epoch {epoch:02d}] Loss: {total_loss/num_batches:.4f} | Val Recall: {val_res['Recall']:.4f} | NDCG: {val_res['NDCG']:.4f}\")\n",
    "\n",
    "    if val_res['Recall'] > best_recall:\n",
    "        best_recall = val_res['Recall']\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"   >>> Best Model Updated!\")\n",
    "\n",
    "print(f\"\\n\u2728 \ud559\uc2b5 \uc885\ub8cc. Best Epoch: {best_epoch}, Recall: {best_recall:.4f}\")\n",
    "\n",
    "# Final Test\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "test_res = evaluate(model, test_df, k=10)\n",
    "print(\"\\n=== Final Test Result ===\")\n",
    "print(f\"Hit: {test_res['Hit']:.4f}, Prec: {test_res['Prec']:.4f}, Recall: {test_res['Recall']:.4f}, NDCG: {test_res['NDCG']:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVu8LLfQ2EJi",
    "outputId": "cd1d7b22-7c12-4eea-af70-ca8ee3c0fd48"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\ude80 LDA-Only \ud559\uc2b5 \uc2dc\uc791 (L1=1e-05)...\n",
      "[Epoch 01] Loss: 0.6895 | Val Recall: 0.0826 | NDCG: 0.0440\n",
      "   >>> Best Model Updated!\n",
      "[Epoch 02] Loss: 0.6753 | Val Recall: 0.1336 | NDCG: 0.0713\n",
      "   >>> Best Model Updated!\n",
      "[Epoch 03] Loss: 0.6236 | Val Recall: 0.1366 | NDCG: 0.0719\n",
      "   >>> Best Model Updated!\n",
      "[Epoch 04] Loss: 0.5420 | Val Recall: 0.1336 | NDCG: 0.0702\n",
      "[Epoch 05] Loss: 0.4829 | Val Recall: 0.1246 | NDCG: 0.0662\n",
      "[Epoch 06] Loss: 0.4483 | Val Recall: 0.1246 | NDCG: 0.0658\n",
      "[Epoch 07] Loss: 0.4249 | Val Recall: 0.1291 | NDCG: 0.0669\n",
      "[Epoch 08] Loss: 0.4143 | Val Recall: 0.1351 | NDCG: 0.0698\n",
      "[Epoch 09] Loss: 0.3958 | Val Recall: 0.1336 | NDCG: 0.0694\n",
      "[Epoch 10] Loss: 0.3863 | Val Recall: 0.1351 | NDCG: 0.0705\n",
      "[Epoch 11] Loss: 0.3785 | Val Recall: 0.1381 | NDCG: 0.0716\n",
      "   >>> Best Model Updated!\n",
      "[Epoch 12] Loss: 0.3624 | Val Recall: 0.1366 | NDCG: 0.0730\n",
      "[Epoch 13] Loss: 0.3548 | Val Recall: 0.1321 | NDCG: 0.0721\n",
      "[Epoch 14] Loss: 0.3411 | Val Recall: 0.1411 | NDCG: 0.0754\n",
      "   >>> Best Model Updated!\n",
      "[Epoch 15] Loss: 0.3319 | Val Recall: 0.1366 | NDCG: 0.0739\n",
      "[Epoch 16] Loss: 0.3219 | Val Recall: 0.1381 | NDCG: 0.0749\n",
      "[Epoch 17] Loss: 0.3102 | Val Recall: 0.1351 | NDCG: 0.0744\n",
      "[Epoch 18] Loss: 0.2997 | Val Recall: 0.1366 | NDCG: 0.0735\n",
      "[Epoch 19] Loss: 0.2878 | Val Recall: 0.1321 | NDCG: 0.0719\n",
      "[Epoch 20] Loss: 0.2827 | Val Recall: 0.1336 | NDCG: 0.0714\n",
      "[Epoch 21] Loss: 0.2751 | Val Recall: 0.1351 | NDCG: 0.0726\n",
      "[Epoch 22] Loss: 0.2664 | Val Recall: 0.1426 | NDCG: 0.0751\n",
      "   >>> Best Model Updated!\n",
      "[Epoch 23] Loss: 0.2590 | Val Recall: 0.1441 | NDCG: 0.0759\n",
      "   >>> Best Model Updated!\n",
      "[Epoch 24] Loss: 0.2520 | Val Recall: 0.1502 | NDCG: 0.0770\n",
      "   >>> Best Model Updated!\n",
      "[Epoch 25] Loss: 0.2485 | Val Recall: 0.1471 | NDCG: 0.0753\n",
      "[Epoch 26] Loss: 0.2371 | Val Recall: 0.1426 | NDCG: 0.0733\n",
      "[Epoch 27] Loss: 0.2339 | Val Recall: 0.1396 | NDCG: 0.0722\n",
      "[Epoch 28] Loss: 0.2284 | Val Recall: 0.1396 | NDCG: 0.0721\n",
      "[Epoch 29] Loss: 0.2230 | Val Recall: 0.1366 | NDCG: 0.0712\n",
      "[Epoch 30] Loss: 0.2180 | Val Recall: 0.1366 | NDCG: 0.0714\n",
      "[Epoch 31] Loss: 0.2153 | Val Recall: 0.1336 | NDCG: 0.0707\n",
      "[Epoch 32] Loss: 0.2093 | Val Recall: 0.1351 | NDCG: 0.0711\n",
      "[Epoch 33] Loss: 0.2036 | Val Recall: 0.1351 | NDCG: 0.0715\n",
      "[Epoch 34] Loss: 0.2017 | Val Recall: 0.1336 | NDCG: 0.0709\n",
      "[Epoch 35] Loss: 0.1958 | Val Recall: 0.1321 | NDCG: 0.0705\n",
      "[Epoch 36] Loss: 0.1930 | Val Recall: 0.1321 | NDCG: 0.0706\n",
      "[Epoch 37] Loss: 0.1912 | Val Recall: 0.1306 | NDCG: 0.0704\n",
      "[Epoch 38] Loss: 0.1834 | Val Recall: 0.1291 | NDCG: 0.0704\n",
      "[Epoch 39] Loss: 0.1822 | Val Recall: 0.1261 | NDCG: 0.0695\n",
      "[Epoch 40] Loss: 0.1807 | Val Recall: 0.1246 | NDCG: 0.0691\n",
      "[Epoch 41] Loss: 0.1751 | Val Recall: 0.1246 | NDCG: 0.0692\n",
      "[Epoch 42] Loss: 0.1710 | Val Recall: 0.1246 | NDCG: 0.0698\n",
      "[Epoch 43] Loss: 0.1720 | Val Recall: 0.1231 | NDCG: 0.0688\n",
      "[Epoch 44] Loss: 0.1686 | Val Recall: 0.1231 | NDCG: 0.0688\n",
      "[Epoch 45] Loss: 0.1637 | Val Recall: 0.1231 | NDCG: 0.0695\n",
      "[Epoch 46] Loss: 0.1618 | Val Recall: 0.1261 | NDCG: 0.0693\n",
      "[Epoch 47] Loss: 0.1589 | Val Recall: 0.1291 | NDCG: 0.0702\n",
      "[Epoch 48] Loss: 0.1570 | Val Recall: 0.1306 | NDCG: 0.0696\n",
      "[Epoch 49] Loss: 0.1553 | Val Recall: 0.1321 | NDCG: 0.0703\n",
      "[Epoch 50] Loss: 0.1551 | Val Recall: 0.1291 | NDCG: 0.0691\n",
      "\n",
      "\u2728 \ud559\uc2b5 \uc885\ub8cc. Best Epoch: 24, Recall: 0.1502\n",
      "\n",
      "=== Final Test Result ===\n",
      "Hit: 0.1502, Prec: 0.0150, Recall: 0.1502, NDCG: 0.0844\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "nhKijO_s3ynd"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}